{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This file contains evaluation methods that take in a set of predicted labels \n",
    "        and a set of ground truth labels and calculate precision, recall, accuracy, f1, and metrics @k\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import *\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def all_metrics(yhat, y, k=8, yhat_raw=None, calc_auc=True):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            yhat: binary predictions matrix \n",
    "            y: binary ground truth matrix\n",
    "            k: for @k metrics\n",
    "            yhat_raw: prediction scores matrix (floats)\n",
    "        Outputs:\n",
    "\n",
    "            dict holding relevant metrics\n",
    "    \"\"\"\n",
    "    names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
    "\n",
    "    #macro\n",
    "    macro = all_macro(yhat, y)\n",
    "\n",
    "    #micro\n",
    "    ymic = y.ravel()\n",
    "    yhatmic = yhat.ravel()\n",
    "    micro = all_micro(yhatmic, ymic)\n",
    "\n",
    "    metrics = {names[i] + \"_macro\": macro[i] for i in range(len(macro))}\n",
    "    metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
    "\n",
    "    #AUC and @k\n",
    "    if yhat_raw is not None and calc_auc:\n",
    "        #allow k to be passed as int or list\n",
    "        if type(k) != list:\n",
    "            k = [k]\n",
    "        for k_i in k:\n",
    "            rec_at_k = recall_at_k(yhat_raw, y, k_i)\n",
    "            metrics['rec_at_%d' % k_i] = rec_at_k\n",
    "            prec_at_k = precision_at_k(yhat_raw, y, k_i)\n",
    "            metrics['prec_at_%d' % k_i] = prec_at_k\n",
    "            metrics['f1_at_%d' % k_i] = 2*(prec_at_k*rec_at_k)/(prec_at_k+rec_at_k)\n",
    "\n",
    "        roc_auc = auc_metrics(yhat_raw, y, ymic)\n",
    "        metrics.update(roc_auc)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def all_macro(yhat, y):\n",
    "    return macro_accuracy(yhat, y), macro_precision(yhat, y), macro_recall(yhat, y), macro_f1(yhat, y)\n",
    "\n",
    "def all_micro(yhatmic, ymic):\n",
    "    return micro_accuracy(yhatmic, ymic), micro_precision(yhatmic, ymic), micro_recall(yhatmic, ymic), micro_f1(yhatmic, ymic)\n",
    "\n",
    "#########################################################################\n",
    "#MACRO METRICS: calculate metric for each label and average across labels\n",
    "#########################################################################\n",
    "\n",
    "def macro_accuracy(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (union_size(yhat, y, 0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "def macro_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "def macro_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
    "    return np.mean(num)\n",
    "\n",
    "def macro_f1(yhat, y):\n",
    "    prec = macro_precision(yhat, y)\n",
    "    rec = macro_recall(yhat, y)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "  \n",
    "###################\n",
    "# INSTANCE-AVERAGED\n",
    "###################\n",
    "\n",
    "def inst_precision(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / yhat.sum(axis=1)\n",
    "    #correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "def inst_recall(yhat, y):\n",
    "    num = intersect_size(yhat, y, 1) / y.sum(axis=1)\n",
    "    #correct for divide-by-zeros\n",
    "    num[np.isnan(num)] = 0.\n",
    "    return np.mean(num)\n",
    "\n",
    "def inst_f1(yhat, y):\n",
    "    prec = inst_precision(yhat, y)\n",
    "    rec = inst_recall(yhat, y)\n",
    "    f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "##############\n",
    "# AT-K\n",
    "##############\n",
    "\n",
    "def recall_at_k(yhat_raw, y, k):\n",
    "    #num true labels in top k predictions / num true labels\n",
    "    sortd = np.argsort(yhat_raw)[:,::-1]\n",
    "    topk = sortd[:,:k]\n",
    "\n",
    "    #get recall at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        num_true_in_top_k = y[i,tk].sum()\n",
    "        denom = y[i,:].sum()\n",
    "        vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    vals = np.array(vals)\n",
    "    vals[np.isnan(vals)] = 0.\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "def precision_at_k(yhat_raw, y, k):\n",
    "    #num true labels in top k predictions / k\n",
    "    sortd = np.argsort(yhat_raw)[:,::-1]\n",
    "    topk = sortd[:,:k]\n",
    "\n",
    "    #get precision at k for each example\n",
    "    vals = []\n",
    "    for i, tk in enumerate(topk):\n",
    "        if len(tk) > 0:\n",
    "            num_true_in_top_k = y[i,tk].sum()\n",
    "            denom = len(tk)\n",
    "            vals.append(num_true_in_top_k / float(denom))\n",
    "\n",
    "    return np.mean(vals)\n",
    "\n",
    "##########################################################################\n",
    "#MICRO METRICS: treat every prediction as an individual binary prediction\n",
    "##########################################################################\n",
    "\n",
    "def micro_accuracy(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / union_size(yhatmic, ymic, 0)\n",
    "\n",
    "def micro_precision(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n",
    "\n",
    "def micro_recall(yhatmic, ymic):\n",
    "    return intersect_size(yhatmic, ymic, 0) / ymic.sum(axis=0)\n",
    "\n",
    "def micro_f1(yhatmic, ymic):\n",
    "    prec = micro_precision(yhatmic, ymic)\n",
    "    rec = micro_recall(yhatmic, ymic)\n",
    "    if prec + rec == 0:\n",
    "        f1 = 0.\n",
    "    else:\n",
    "        f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def auc_metrics(yhat_raw, y, ymic):\n",
    "    if yhat_raw.shape[0] <= 1:\n",
    "        return\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    #get AUC for each label individually\n",
    "    relevant_labels = []\n",
    "    auc_labels = {}\n",
    "    for i in range(y.shape[1]):\n",
    "        #only if there are true positives for this label\n",
    "        if y[:,i].sum() > 0:\n",
    "            fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
    "            if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
    "                auc_score = auc(fpr[i], tpr[i])\n",
    "                if not np.isnan(auc_score): \n",
    "                    auc_labels[\"auc_%d\" % i] = auc_score\n",
    "                    relevant_labels.append(i)\n",
    "\n",
    "    #macro-AUC: just average the auc scores\n",
    "    aucs = []\n",
    "    for i in relevant_labels:\n",
    "        aucs.append(auc_labels['auc_%d' % i])\n",
    "    roc_auc['auc_macro'] = np.mean(aucs)\n",
    "\n",
    "    #micro-AUC: just look at each individual prediction\n",
    "    yhatmic = yhat_raw.ravel()\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic) \n",
    "    roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "########################\n",
    "# METRICS BY CODE TYPE\n",
    "########################\n",
    "\n",
    "def results_by_type(Y, mdir, version='mimic3'):\n",
    "    d2ind = {}\n",
    "    p2ind = {}\n",
    "\n",
    "    #get predictions for diagnoses and procedures\n",
    "    diag_preds = defaultdict(lambda: set([]))\n",
    "    proc_preds = defaultdict(lambda: set([]))\n",
    "    preds = defaultdict(lambda: set())\n",
    "    with open('%s/preds_test.psv' % mdir, 'r') as f:\n",
    "        r = csv.reader(f, delimiter='|')\n",
    "        for row in r:\n",
    "            if len(row) > 1:\n",
    "                for code in row[1:]:\n",
    "                    preds[row[0]].add(code)\n",
    "                    if code != '':\n",
    "                        try:\n",
    "                            pos = code.index('.')\n",
    "                            if pos == 3 or (code[0] == 'E' and pos == 4):\n",
    "                                if code not in d2ind:\n",
    "                                    d2ind[code] = len(d2ind)\n",
    "                                diag_preds[row[0]].add(code)\n",
    "                            elif pos == 2:\n",
    "                                if code not in p2ind:\n",
    "                                    p2ind[code] = len(p2ind)\n",
    "                                proc_preds[row[0]].add(code)\n",
    "                        except:\n",
    "                            if len(code) == 3 or (code[0] == 'E' and len(code) == 4):\n",
    "                                if code not in d2ind:\n",
    "                                    d2ind[code] = len(d2ind)\n",
    "                                diag_preds[row[0]].add(code)\n",
    "    #get ground truth for diagnoses and procedures\n",
    "    diag_golds = defaultdict(lambda: set([]))\n",
    "    proc_golds = defaultdict(lambda: set([]))\n",
    "    golds = defaultdict(lambda: set())\n",
    "    test_file = '%s/test_%s.csv' % (PROJECT_DIR, str(Y)) if version == 'mimic3' else '%s/test.csv' % MIMIC_2_DIR\n",
    "    with open(test_file, 'r') as f:\n",
    "        r = csv.reader(f)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            codes = set([c for c in row[3].split(';')])\n",
    "            for code in codes:\n",
    "                golds[row[1]].add(code)\n",
    "                try:\n",
    "                    pos = code.index('.')\n",
    "                    if pos == 3:\n",
    "                        if code not in d2ind:\n",
    "                            d2ind[code] = len(d2ind)\n",
    "                        diag_golds[row[1]].add(code)\n",
    "                    elif pos == 2:\n",
    "                        if code not in p2ind:\n",
    "                            p2ind[code] = len(p2ind)\n",
    "                        proc_golds[row[1]].add(code)\n",
    "                except:\n",
    "                    if len(code) == 3 or (code[0] == 'E' and len(code) == 4):\n",
    "                        if code not in d2ind:\n",
    "                            d2ind[code] = len(d2ind)\n",
    "                        diag_golds[row[1]].add(code)\n",
    "\n",
    "    hadm_ids = sorted(set(diag_golds.keys()).intersection(set(diag_preds.keys())))\n",
    "\n",
    "    ind2d = {i:d for d,i in d2ind.items()}\n",
    "    ind2p = {i:p for p,i in p2ind.items()}\n",
    "    type_dicts = (ind2d, ind2p)\n",
    "    return diag_preds, diag_golds, proc_preds, proc_golds, golds, preds, hadm_ids, type_dicts\n",
    "\n",
    "\n",
    "def diag_f1(diag_preds, diag_golds, ind2d, hadm_ids):\n",
    "    num_labels = len(ind2d)\n",
    "    yhat_diag = np.zeros((len(hadm_ids), num_labels))\n",
    "    y_diag = np.zeros((len(hadm_ids), num_labels))\n",
    "    for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "        yhat_diag_inds = [1 if ind2d[j] in diag_preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        gold_diag_inds = [1 if ind2d[j] in diag_golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        yhat_diag[i] = yhat_diag_inds\n",
    "        y_diag[i] = gold_diag_inds\n",
    "    return micro_f1(yhat_diag.ravel(), y_diag.ravel())\n",
    "\n",
    "def proc_f1(proc_preds, proc_golds, ind2p, hadm_ids):\n",
    "    num_labels = len(ind2p)\n",
    "    yhat_proc = np.zeros((len(hadm_ids), num_labels))\n",
    "    y_proc = np.zeros((len(hadm_ids), num_labels))\n",
    "    for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "        yhat_proc_inds = [1 if ind2p[j] in proc_preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        gold_proc_inds = [1 if ind2p[j] in proc_golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        yhat_proc[i] = yhat_proc_inds\n",
    "        y_proc[i] = gold_proc_inds\n",
    "    return micro_f1(yhat_proc.ravel(), y_proc.ravel())\n",
    "\n",
    "def metrics_from_dicts(preds, golds, mdir, ind2c):\n",
    "    with open('%s/pred_100_scores_test.json' % mdir, 'r') as f:\n",
    "        scors = json.load(f)\n",
    "\n",
    "    hadm_ids = sorted(set(golds.keys()).intersection(set(preds.keys())))\n",
    "    num_labels = len(ind2c)\n",
    "    yhat = np.zeros((len(hadm_ids), num_labels))\n",
    "    yhat_raw = np.zeros((len(hadm_ids), num_labels))\n",
    "    y = np.zeros((len(hadm_ids), num_labels))\n",
    "    for i,hadm_id in tqdm(enumerate(hadm_ids)):\n",
    "        yhat_inds = [1 if ind2c[j] in preds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        yhat_raw_inds = [scors[hadm_id][ind2c[j]] if ind2c[j] in scors[hadm_id] else 0 for j in range(num_labels)]\n",
    "        gold_inds = [1 if ind2c[j] in golds[hadm_id] else 0 for j in range(num_labels)]\n",
    "        yhat[i] = yhat_inds\n",
    "        yhat_raw[i] = yhat_raw_inds\n",
    "        y[i] = gold_inds\n",
    "    return yhat, yhat_raw, y, all_metrics(yhat, y, yhat_raw=yhat_raw, calc_auc=False)\n",
    "\n",
    "\n",
    "def union_size(yhat, y, axis):\n",
    "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "def intersect_size(yhat, y, axis):\n",
    "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
    "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print()\n",
    "    if \"auc_macro\" in metrics.keys():\n",
    "        print(\"[MACRO] accuracy, precision, recall, f-measure, AUC\")\n",
    "        print(\"%.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"], metrics[\"auc_macro\"]))\n",
    "    else:\n",
    "        print(\"[MACRO] accuracy, precision, recall, f-measure\")\n",
    "        print(\"%.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"]))\n",
    "\n",
    "    if \"auc_micro\" in metrics.keys():\n",
    "        print(\"[MICRO] accuracy, precision, recall, f-measure, AUC\")\n",
    "        print(\"%.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"], metrics[\"auc_micro\"]))\n",
    "    else:\n",
    "        print(\"[MICRO] accuracy, precision, recall, f-measure\")\n",
    "        print(\"%.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"]))\n",
    "    for metric, val in metrics.items():\n",
    "        if metric.find(\"rec_at\") != -1:\n",
    "            print(\"%s: %.4f\" % (metric, val))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/lixc/Downloads/data/caml-mimic/mimicdata/mimic3/train_50.csv\"\n",
    "Y = 50\n",
    "version = 'mimic3'\n",
    "mdir  = \"/home/lixc/Documents/dl/mimic/conv_attn_Jan_22_13:58:59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2c, _ = datasets.load_full_codes(train_path, version=version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {0: '038.9',\n",
       "             1: '244.9',\n",
       "             2: '250.00',\n",
       "             3: '272.0',\n",
       "             4: '272.4',\n",
       "             5: '276.1',\n",
       "             6: '276.2',\n",
       "             7: '285.1',\n",
       "             8: '285.9',\n",
       "             9: '287.5',\n",
       "             10: '305.1',\n",
       "             11: '311',\n",
       "             12: '33.24',\n",
       "             13: '36.15',\n",
       "             14: '37.22',\n",
       "             15: '37.23',\n",
       "             16: '38.91',\n",
       "             17: '38.93',\n",
       "             18: '39.61',\n",
       "             19: '39.95',\n",
       "             20: '401.9',\n",
       "             21: '403.90',\n",
       "             22: '410.71',\n",
       "             23: '412',\n",
       "             24: '414.01',\n",
       "             25: '424.0',\n",
       "             26: '427.31',\n",
       "             27: '428.0',\n",
       "             28: '45.13',\n",
       "             29: '486',\n",
       "             30: '496',\n",
       "             31: '507.0',\n",
       "             32: '511.9',\n",
       "             33: '518.81',\n",
       "             34: '530.81',\n",
       "             35: '584.9',\n",
       "             36: '585.9',\n",
       "             37: '599.0',\n",
       "             38: '88.56',\n",
       "             39: '88.72',\n",
       "             40: '96.04',\n",
       "             41: '96.6',\n",
       "             42: '96.71',\n",
       "             43: '96.72',\n",
       "             44: '99.04',\n",
       "             45: '99.15',\n",
       "             46: '995.92',\n",
       "             47: 'V15.82',\n",
       "             48: 'V45.81',\n",
       "             49: 'V58.61'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1729it [00:00, 53091.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MACRO] accuracy, precision, recall, f-measure\n",
      "0.3950, 0.6188, 0.4847, 0.5436\n",
      "[MICRO] accuracy, precision, recall, f-measure\n",
      "0.4339, 0.6919, 0.5377, 0.6052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "diag_preds, diag_golds, proc_preds, proc_golds, golds, preds, hadm_ids, type_dicts = results_by_type(Y, mdir, version)\n",
    "yhat, yhat_raw, y, metrics = metrics_from_dicts(preds, golds, mdir, ind2c)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]),\n",
       " array([[1.30313724e-01, 3.74005642e-03, 4.66462404e-01, ...,\n",
       "         8.85173231e-02, 5.96550526e-04, 9.16932344e-01],\n",
       "        [6.73456361e-06, 3.92973307e-04, 4.19689387e-01, ...,\n",
       "         2.33186893e-02, 3.26277502e-02, 1.18286218e-04],\n",
       "        [8.88882550e-06, 6.79206278e-04, 1.79657564e-02, ...,\n",
       "         3.13825943e-02, 4.50870080e-04, 1.11506262e-03],\n",
       "        ...,\n",
       "        [9.66994412e-06, 3.01297201e-04, 2.94511649e-03, ...,\n",
       "         8.53539035e-02, 4.61486401e-04, 4.24941507e-04],\n",
       "        [1.18929399e-02, 8.37988977e-04, 9.24891210e-04, ...,\n",
       "         7.53900409e-02, 3.90289119e-03, 7.43460841e-03],\n",
       "        [4.21904355e-01, 5.68538129e-01, 8.80471081e-04, ...,\n",
       "         3.86035517e-02, 3.18047154e-04, 3.31593328e-03]]),\n",
       " array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat, yhat_raw,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION@8: 0.4878\n",
      "PRECISION@15: 0.3326\n"
     ]
    }
   ],
   "source": [
    "k = [5] if Y == '50' else [8,15]\n",
    "prec_at_8 = precision_at_k(yhat_raw, y, k=8)\n",
    "print(\"PRECISION@8: %.4f\" % prec_at_8)\n",
    "prec_at_15 = precision_at_k(yhat_raw, y, k=15)\n",
    "print(\"PRECISION@15: %.4f\" % prec_at_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1564it [00:00, 91787.81it/s]\n",
      "1564it [00:00, 119872.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BY CODE TYPE] f1-diag f1-proc\n",
      "0.6197 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_diag = diag_f1(diag_preds, diag_golds, type_dicts[0], hadm_ids)\n",
    "f1_proc = proc_f1(proc_preds, proc_golds, type_dicts[1], hadm_ids)\n",
    "print(\"[BY CODE TYPE] f1-diag f1-proc\")\n",
    "print(\"%.4f %.4f\" % (f1_diag, f1_proc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
